{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035871cc-dd87-4873-85d2-d935f890493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset,Dataset\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "# Initialize the accelerator for mixed precision\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ed3a1f-8fbc-495b-b6ea-c1c22108ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f0b248-3327-4c15-8bc6-5716976b8f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the VQAv2 dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the MMMU dataset\n",
    "print(\"Loading the VQAv2 dataset...\")\n",
    "# Update this line with the correct dataset source if available on Hugging Face\n",
    "# Replace with your local dataset path if needed\n",
    "try:\n",
    "    streamed_dataset = load_dataset(\"lmms-lab/VQAv2\", split = \"validation\", streaming=True)\n",
    "except:\n",
    "    raise ValueError(\"MMMU dataset not found. Ensure it's correctly downloaded or accessible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7a617f-af89-4054-802f-1dc51e992964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sample 1000 entries from the dataset\n",
    "num_samples = 50\n",
    "# samples = dataset.shuffle(seed=40).select(range(num_samples))\n",
    "samples1 = list(itertools.islice(streamed_dataset, num_samples))\n",
    "\n",
    "# Step 3: Convert the list of samples to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(samples1)\n",
    "samples = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1381e8d9-19e2-46d7-b426-85d09b2b6445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.95s/it]\n",
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "model = accelerator.prepare(model)\n",
    "tokenizer = processor.tokenizer  # Use the tokenizer from the processor\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4beacd1d-975d-4531-848a-a131fc0c952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     device_map=\"auto\",\n",
    "#     use_cache=None,\n",
    "#     attn_implementation=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae70f2c2-5c53-4f7b-9b2a-004c486ebfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Step 3: Define a function to generate text\n",
    "def generate_formatted_response(tokenizer, model, prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=1,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the text between the last \"### START ###\" and the next \"### END ###\"\n",
    "    start_delimiter = \"### START ###\"\n",
    "    end_delimiter = \"### END ###\"\n",
    "    \n",
    "    # Find the last occurrence of the start delimiter\n",
    "    last_start_index = response.rfind(start_delimiter)\n",
    "    if last_start_index != -1:\n",
    "        last_start_index += len(start_delimiter)\n",
    "        end_index = response.find(end_delimiter, last_start_index)\n",
    "        \n",
    "        # Extract text if both delimiters are found\n",
    "        if end_index != -1:\n",
    "            extracted_text = response[last_start_index:end_index].strip()\n",
    "            return extracted_text\n",
    "    \n",
    "    return \"No content found between delimiters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3f925fd-f1e5-44c2-b2c0-945fd561092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute exact match accuracy\n",
    "def exact_match(pred, target):\n",
    "    return int(pred.strip().lower() == target.strip().lower())\n",
    "\n",
    "# Initialize variables for accuracy calculation\n",
    "total_predictions = 0\n",
    "correct_predictions_orig = 0\n",
    "correct_predictions_paraphrase = 0\n",
    "data = []  # To store data for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2fce1bdf-588e-46b1-a5c7-743bf2d34680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_para = pd.read_csv(\"./Paraphrased_Questions.csv\")\n",
    "def find_value(df, search_value, column_to_search, column_to_return):\n",
    "    \"\"\"\n",
    "    Searches for a value in column_to_search and returns the corresponding value\n",
    "    from column_to_return. If not found, raises a ValueError.\n",
    "    \"\"\"\n",
    "    result = df.loc[df[column_to_search] == search_value, column_to_return]\n",
    "    if not result.empty:\n",
    "        return result.values[0]  # Return the first matching value\n",
    "    else:\n",
    "        raise ValueError(f\"Error: Value '{search_value}' not found in column '{column_to_search}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "940f9e46-e54d-46a5-8500-1d7dfd54987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, var=0.01):\n",
    "    # Load the image using Pillow\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    image = np.array(image).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    noise = np.random.normal(mean, var**0.5, image.shape)\n",
    "    \n",
    "    # Add noise to the image\n",
    "    noisy_image = image + noise\n",
    "    \n",
    "    # Clip the image to [0, 1] range\n",
    "    noisy_image = np.clip(noisy_image, 0, 1)\n",
    "    \n",
    "    # Convert back to uint8 format\n",
    "    noisy_image = (noisy_image * 255).astype(np.uint8)\n",
    "    \n",
    "    return noisy_image\n",
    "\n",
    "def add_salt_and_pepper_noise(image, amount=0.05):\n",
    "    # Ensure the image is a NumPy array\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        image = np.array(image)\n",
    "\n",
    "    # Create a copy of the image\n",
    "    noisy_image = np.copy(image)\n",
    "\n",
    "    # Calculate the number of salt and pepper pixels\n",
    "    num_salt = int(amount * image.size * 0.5)\n",
    "    num_pepper = int(amount * image.size * 0.5)\n",
    "\n",
    "    # Add salt (white) noise\n",
    "    coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape]\n",
    "    noisy_image[coords[0], coords[1], :] = 255\n",
    "\n",
    "    # Add pepper (black) noise\n",
    "    coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape]\n",
    "    noisy_image[coords[0], coords[1], :] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "def make_random_pixels_black_pil(image, amount=0.05):\n",
    "    \"\"\"\n",
    "    Adds random black pixels to a PIL image.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image.Image): Input image.\n",
    "    - amount (float): Fraction of pixels to turn black.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Image with random black pixels.\n",
    "    \"\"\"\n",
    "    # Convert PIL image to NumPy array\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Create a copy of the image to modify\n",
    "    noisy_image = np.copy(image_np)\n",
    "\n",
    "    # Calculate the number of pixels to turn black\n",
    "    num_black = int(amount * image_np.size / image_np.shape[2])  # Adjust for channels\n",
    "\n",
    "    # Generate random coordinates for black pixels\n",
    "    coords = [np.random.randint(0, dim, num_black) for dim in image_np.shape[:2]]\n",
    "\n",
    "    # Set those pixels to black\n",
    "    noisy_image[coords[0], coords[1]] = [0, 0, 0]  # Black in RGB\n",
    "\n",
    "    # Convert back to PIL image\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "def rotate_image_pil(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates a PIL image by a specified angle.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image.Image): Input image.\n",
    "    - angle (float): Angle in degrees to rotate the image. Positive values for counterclockwise rotation.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Rotated image.\n",
    "    \"\"\"\n",
    "    return image.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "\n",
    "def horizontal_flip_pil(image):\n",
    "    \"\"\"\n",
    "    Flips a PIL image horizontally.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image.Image): Input image.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Horizontally flipped image.\n",
    "    \"\"\"\n",
    "    return image.transpose(Image.FLIP_LEFT_RIGHT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c149ef27-fea3-4111-975e-c89ea24569b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for a,sample in enumerate(samples):\n",
    "    print(a)\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"multiple_choice_answer\"]\n",
    "    \n",
    "    image = sample['image']\n",
    "    image.save(f\"./vqa_image/{a+1}.jpg\")\n",
    "    if image.mode != \"RGB\":\n",
    "        print(f\"Image mode is {image.mode}. Converting to RGB.\")\n",
    "        image = image.convert(\"RGB\")\n",
    "    image_noise = rotate_image_pil(image, 45)\n",
    "    # image_noise = add_gaussian_noise(image)\n",
    "    # image_noise = make_random_pixels_black(image)\n",
    "    \n",
    "    # #########Para\n",
    "    # # Example usage\n",
    "    # prompt = \"\"\"\n",
    "    # Rephrase the query given but provide only one alternative that means the same.\n",
    "    \n",
    "    # Please respond with only the rephrased sentence strictly between the delimiters.\n",
    "    \n",
    "    # Example:\n",
    "    # Query: How can I improve my code?\n",
    "    # ### START ###\n",
    "    # How can I enhance my code?\n",
    "    # ### END ###\n",
    "    \n",
    "    # Now, rephrase the given query:\n",
    "    \n",
    "    # Query: {}\n",
    "    # ### START ###\n",
    "    # \"\"\".format(question)\n",
    "    \n",
    "    \n",
    "    # rephrased_llama = generate_formatted_response(tokenizer_llama, model_llama,prompt)\n",
    "    rephrased_llama = find_value(df_para, question, \"Original question\",\"Paraphrased Questions\")\n",
    "    \n",
    "\n",
    "    #############\n",
    "    \n",
    "    conversation_1 = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Answer within 3 words maximum:- \" + question},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    conversation_2 = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Answer within 3 words maximum:- \" + rephrased_llama},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We can simply feed images in the order they have to be used in the text prompt\n",
    "    inputs = processor(images=image, text=prompt_1, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "    \n",
    "    \n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=6)\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "    assistant_response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "\n",
    "    if assistant_response:\n",
    "        total_predictions += 1\n",
    "        # print(\"Prompt:- \" + question)\n",
    "        # print(\"Assistant:- \" + assistant_response)\n",
    "        # print(\"Answer:- \" + answer)\n",
    "        correct_predictions_orig += exact_match(assistant_response, answer)\n",
    "    var3 = assistant_response\n",
    "    prompt_1 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n",
    "    # We can simply feed images in the order they have to be used in the text prompt\n",
    "    inputs = processor(images=image_noise, text=prompt_1, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=6)\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "    assistant_response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    data.append([question, rephrased_llama, var3, assistant_response, answer]) \n",
    "    if assistant_response:\n",
    "        correct_predictions_paraphrase += exact_match(assistant_response, answer)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d345af9-0502-44ce-8b75-fd9f605fc84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93d45dad-57b7-48d9-8a33-d346041dc617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions_orig/total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ed33fe7-b224-4d9b-b589-4262b974db3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions_paraphrase/total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66f06708-99c1-4114-9b36-cf99645bfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=[\"Original question\", \"Paraphrased question\", \"Model response-orig\", \"Model-response-para\", \"answer\"])\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df.to_excel(\"./output_paraphrased_rotate_45.xlsx\", index=False, engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a25c77f2-6686-46f9-9fbd-908b84eff47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: vqa_image/ (stored 0%)\n",
      "  adding: vqa_image/5.jpg (deflated 0%)\n",
      "  adding: vqa_image/7.jpg (deflated 0%)\n",
      "  adding: vqa_image/17.jpg (deflated 0%)\n",
      "  adding: vqa_image/10.jpg (deflated 0%)\n",
      "  adding: vqa_image/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: vqa_image/.ipynb_checkpoints/1-checkpoint.jpg (deflated 7%)\n",
      "  adding: vqa_image/40.jpg (deflated 2%)\n",
      "  adding: vqa_image/31.jpg (deflated 2%)\n",
      "  adding: vqa_image/18.jpg (deflated 0%)\n",
      "  adding: vqa_image/26.jpg (deflated 2%)\n",
      "  adding: vqa_image/15.jpg (deflated 0%)\n",
      "  adding: vqa_image/41.jpg (deflated 2%)\n",
      "  adding: vqa_image/22.jpg (deflated 2%)\n",
      "  adding: vqa_image/12.jpg (deflated 0%)\n",
      "  adding: vqa_image/13.jpg (deflated 0%)\n",
      "  adding: vqa_image/33.jpg (deflated 2%)\n",
      "  adding: vqa_image/32.jpg (deflated 2%)\n",
      "  adding: vqa_image/35.jpg (deflated 2%)\n",
      "  adding: vqa_image/36.jpg (deflated 2%)\n",
      "  adding: vqa_image/27.jpg (deflated 2%)\n",
      "  adding: vqa_image/30.jpg (deflated 2%)\n",
      "  adding: vqa_image/2.jpg (deflated 7%)\n",
      "  adding: vqa_image/8.jpg (deflated 0%)\n",
      "  adding: vqa_image/29.jpg (deflated 2%)\n",
      "  adding: vqa_image/39.jpg (deflated 2%)\n",
      "  adding: vqa_image/48.jpg (deflated 2%)\n",
      "  adding: vqa_image/34.jpg (deflated 2%)\n",
      "  adding: vqa_image/4.jpg (deflated 0%)\n",
      "  adding: vqa_image/3.jpg (deflated 7%)\n",
      "  adding: vqa_image/28.jpg (deflated 2%)\n",
      "  adding: vqa_image/49.jpg (deflated 1%)\n",
      "  adding: vqa_image/16.jpg (deflated 0%)\n",
      "  adding: vqa_image/25.jpg (deflated 2%)\n",
      "  adding: vqa_image/9.jpg (deflated 0%)\n",
      "  adding: vqa_image/6.jpg (deflated 0%)\n",
      "  adding: vqa_image/14.jpg (deflated 0%)\n",
      "  adding: vqa_image/42.jpg (deflated 2%)\n",
      "  adding: vqa_image/47.jpg (deflated 2%)\n",
      "  adding: vqa_image/46.jpg (deflated 2%)\n",
      "  adding: vqa_image/45.jpg (deflated 2%)\n",
      "  adding: vqa_image/24.jpg (deflated 2%)\n",
      "  adding: vqa_image/11.jpg (deflated 0%)\n",
      "  adding: vqa_image/19.jpg (deflated 2%)\n",
      "  adding: vqa_image/21.jpg (deflated 2%)\n",
      "  adding: vqa_image/1.jpg (deflated 7%)\n",
      "  adding: vqa_image/43.jpg (deflated 2%)\n",
      "  adding: vqa_image/23.jpg (deflated 2%)\n",
      "  adding: vqa_image/37.jpg (deflated 2%)\n",
      "  adding: vqa_image/44.jpg (deflated 2%)\n",
      "  adding: vqa_image/50.jpg (deflated 1%)\n",
      "  adding: vqa_image/38.jpg (deflated 2%)\n",
      "  adding: vqa_image/20.jpg (deflated 2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!zip -r vqa_images.zip ./vqa_image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "876a6d2a-1a97-401b-97e2-68a38d881b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrased Output: How is Martha doing?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Rephrased Output:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff6334-506f-4085-89f6-97b5338a127a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp4",
   "language": "python",
   "name": "nlp4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
